
template: titleslide

# Parallel Computing with Python

---

# Parallel Computing with Python

- Python threads and the Global Interpreter Lock (GIL)

- Threading and interfaced code
  - NumPy/SciPy
  - Cython + OpenMP

- Parallel computing with Numba

- Python `multiprocessing`

- mpi4py

- Python for HPC



*(parts adapted with permission from http://doi.org/10.5281/zenodo.1409686)*

---

template:titleslide
# Python threads and the GIL

---

# Python threads and the GIL

- CPython interpreter process can spawn threads (pthreads), e.g.:

```Python
import threading

def thread_function():
    // do some work

t1 = threading.Thread(target=thread_function)
t2 = threading.Thread(target=thread_function)
t1.start()
t2.start()
```

- Efficient parallelism limited for thread functions containing pure Python code due to the GIL

---
# Python threads and the GIL

GIL = Global Interpreter Lock
(https://github.com/python/cpython/blob/main/Python/ceval_gil.c)

-  **mutex lock**:
    - Only the thread that holds the GIL can execute Python bytecode 
    - Other threads sleep and periodically try to acquire the GIL
        - some may bypass the GIL and execute machine code independently of the interpreter

- Early implementations: CPU-bound compute-intensive threads wasted vast amounts of time 'battling' to obtain the GIL (massive contention)

- Newer (Python >3.2) implementations more efficient, but still expect diminishing returns for CPU-intensive multithreading

---

# Why the GIL?

- Core CPython component

- Created to ensure thread safety for Python garbage collection
    - avoids race condition on reference counter

- Historically useful for integrating C code that was not itself guaranteed to be thread safe into CPython 

- Easier to execute single-threaded programs fast (no need to acquire or release locks on all data structures separately)

- Easier to implement than lock-free interpreter or finer-grained locks!

- Good enough for original purpose (*not* parallel numerical computing)

---

# Releasing the GIL

- Non-Python code called from Python code can release the GIL if it obeys the following criteria:
    - **doesn't** operate on Python objects
    - **doesn't** call Python/C API functions
    - **doesn't** have side effects for other threads (e.g. modify global variables)

- Examples:
    - CPython (C) function code that does I/O from/to underlying C buffer
        - i.e. before creating or after unpacking containing Python object 
    - NumPy functions that only operate on underlying C arrays, e.g.:
        - NumPy > 1.9.0: array indexing (slicing, broadcast etc.)
        - NumPy > 1.12.0: all generalized ufuncs, most linear algebra

- Code that releases the GIL can, if thread safe, be executed: 
    - concurrently with itself (multiple Python threads)
    - concurrently with other GIL-releasing code (multiple Python threads)
    - concurrently with Python bytecode (one other Python thread only)

---

# Releasing the GIL

- Other imported CPython extension modules - generated by Cython or by interfacing existing Fortran/C/C++ code - may also release the GIL
    - Developer needs to ensure thread safety

- Even without releasing GIL, can call external non-Python code that runs multithreaded (using OpenMP, pthreads, ...) 
    - e.g. NumPy/SciPy function called by single Python thread may call OpenMP-threaded high-performance maths libraries

example: matrix product
```
>>> C = numpy.dot(A,B)

```
executed by threaded BLAS library if NumPy linked to BLAS at build time

- Next look at how we can use Cython to create Python extension modules that incorporate OpenMP threaded parallelism


---

template:titleslide
# Cython + OpenMP

---

# Cython + OpenMP

`cython.parallel` module brings OpenMP runtime and thread control to Cython
 - **`cython.parallel.prange`** generates **`#pragma omp for`**  in C code 

Example reduction:

```Python
# code saved in file ending in .pyx for cython code
from cython.parallel import prange

# First declare the variables we are going to use with cdefs:
cdef int i
cdef int n = 10000
cdef int sum = 0

# Use prange instead of native Python's range
for i in prange(n, nogil=True):
    sum += i

print(sum)
```

---

# Cython + OpenMP + NumPy

Thread-parallelise operations involving NumPy arrays using typed memoryviews:

```Cython
from cython.parallel import prange

def func(double[:] x, double alpha):
    cdef int i

    for i in prange(x.shape[0], nogil=True):
        x[i] = alpha * x[i]
```

---

# OpenMP control of Cython parallel region 

- OpenMP automatically starts thread pool and distributes the work according to the chosen schedule (`static`, `dynamic`, `guided`, `runtime`) and chunk size (optional)

- Number of threads specified in function call, OMP_NUM_THREADS, or equal to number of cores available

- Thread-locality (shared/private) and reductions of variables inferred automatically according to OpenMP conventions

- Can use OpenMP API syntax, e.g. `omp_get_thread_num()` as an alternative to `cython.parallel.threadid()`: 

```Python
from cython.parallel cimport parallel
cimport openmp

cdef int num_threads

openmp.omp_set_dynamic(1)
with nogil, parallel():
    num_threads = openmp.omp_get_num_threads()
```

---

# `nogil=True`

- `nogil=True` tells Cython to generate C code that releases the GIL
  - Cython code in parallel region must not manipulate Python objects
     - otherwise Cython compiler will (probably!) complain
  - Must also not *call* any other code that manipulates Python objects 

- Successfully using `nogil=True` is key to performant OpenMP threading
    - Because proxy indicator for "no interaction with Python interpreter"
        - i.e. what Numba calls `nopython=True`
    - No Python interpreter <--> machine code transition overheads
        - would likely destroy multithreading efficiency
    - May care less about concurrent execution with Python threads 

- Programmer responsible for ensuring thread safety!
  - within Cython parallel region
  - within/between any other code called

---

# `nogil=True`
Suppose we want to print out intermediate values:
```Python
# Thread ID
from cython.parallel import prange

cdef int i
cdef int sum = 0

for i in prange(1000, nogil=True):
    sum += i
    print("Current loop iter:", i)
```

- Cython compiler will throw an error!

Why?

- `print` is a Python native function 
- GIL can not be released on code that involves any Python objects

---

# `nogil=True`

- General workaround strategy: `cimport` a C function or data type (GIL-free!) to replace pure Python code in `nogil=True` region
 - e.g. the `printf()` function from C:

```Python
#Thread ID
from cython.parallel import prange
# Give me a C function!
from libc.stdio cimport printf

cdef int i
cdef int sum = 0

for i in prange(4, nogil=True):
    sum += i
    printf("Current loop iter: %d\n", i)
```


---

# Compiling...

To compile parallel Cython code into C need to add `-fopenmp` flags to `setup.py`:

```Python
from setuptools import Extension, setup
from Cython.Build import cythonize

ext_modules = [
    Extension(
        "hello",
        ["hello.pyx"],
        extra_compile_args=['-fopenmp'],
        extra_link_args=['-fopenmp'],
    )
]

setup(
    name='hello-parallel-world',
    ext_modules=cythonize(ext_modules),
)
```


---

# Cython for Parallel Computing

- Relatively mature, providing an opportunity to do efficient thread-based parallel programming when we can guarantee not to use Python objects

- cython.parallel provides interface to much of OpenMP API

- Can stick to the basic constructs like `prange` and `with nogil` to access simple parallelism techniques

- Requires more programming effort time compared to e.g. Numba, but offers finer grained control, interoperability with C and C++ code, and access to some of the power of the OpenMP library. 

#### References:
https://cython.readthedocs.io/en/stable/src/userguide/parallelism.html

---

template:titleslide
# Parallel computing with Numba

---


# Numba parallel

- Numba can attempt to automatically parallelise a jit-decorated function, request using `@jit(parallel=True, nopython=True)`
 - will attempt to optimize array operations and run them in parallel

- `parallel=True` requires `nopython=True`
 - i.e. Numba must be able to do compile-time type inference and not use CPython's C API, including not creating any new Python objects

- Threading layer is implemented using one choice of:
 - OpenMP
 - Intel TBB
 - Numba's internal work queue scheduler

---

# Numba and the GIL

- `@jit(nogil=True)` releases the GIL upon entry to jit-decorated function
    - Allows generated machine code to run concurrently with other Python threads executing Python bytecode and/or Numba-generated machine code for this or other jit-decorated functions

- `nogil=True` logically requires `nopython=True`

- Have to consider thread safety (consistency, synchronization, race conditions, etc.)

---

# Numba parallel

Numba attempts to parallelise:
- common element-wise arithmetic functions between NumPy arrays and between arrays and scalars (e.g. `+`, `-`, `~`, `*`, `**`, `==`, `!=`, `<`)

- NumPy reduction functions (`sum`, `prod`, `min`, `max`, `argmin`, and `argmax`) and array math functions (`mean`, `var`, and `std`)

- NumPy array creation functions (`zeros`, `ones`, `arange`, `linspace`), and several random functions

- NumPy `dot` function between a matrix and a vector, or two vectors

- Above operations for multi-dimensional arrays when operands have matching dimension and size

- Array assignment where the target is a slice and the source another compatible slice or a scalar

- Fusion of adjacent parallel operations to maximize cache locality

---

# Numba parallel

Numba automatically tries to detect loops that can be parallelised
- Can enforce using explicit parallel loop construct `prange`:

```Python
from numba import jit, prange

@jit(nopython=True, parallel=True)
def prange_test(A):   # 'A' would be a 1D numpy array in this example
    s = 0
    for i in prange(A.shape[0]):
        s += A[i]
    return s
```
- infers a reduction if a shared variable is updated by a binary function/operator (+, -, /, *)
 - **`s`** above is automatically identified as a reduction variable

- Developer must beware cross-iteration dependencies and thread safety

- Easier to try auto-parallelisation first, then explicit `prange` if Numba cannot determine automatically if a loop can be parallelised

---

# Numba parallel - summary

- Very easy to implement - just extend JIT decoration of compute-intensive functions

- Some GPU offloading implemented

- Still limited to shared-memory concurrency

---

template:titleslide
# Python `multiprocessing`



---

# Python `multiprocessing`

- `multiprocessing` module creates multiple instances of the Python interpreter, each running as an independent OS-level (sub)process 
 - no GIL contention between instances

- Each Python instance has its own memory space, managed by the OS
 - possible to declare a shared memory between processes, but this brings up synchronisation issues

- Need communication between processes, requires explicit conversion of Python object structure hierarchy into a bytestream ("pickling")
 - Originally designed for file storage of Python objects - slow

- Bytestream sent from one process to another, and reconstituted ("unpickled")

- Approach best suited for independent tasks, or tasks requiring minimal communication

---

# Multiprocessing

- Typically define a task as a Python function

Task distribution / work sharing can use:

- Static data-parallel pool of workers model
 - split up work of predetermined size across available workers/cores
 - useful when workload distribution is static 
 - not adaptable to varying / unpredictable workload
- Queue approach
 - Put data -  wrapped as Python objects - onto one or more work queues
 - Each worker queries queue once done with work to receive more
 - Dynamically adaptable to varying load
 - Communication overheads can dominate, especially due to pickling

- Useful in some (limited) circumstances
    - Can operate across distributed-memory nodes in principle, but tricky in practice - operates via TCP server and needs manual specification of IP addresses - not suitable for HPC deployment

---

# mpi4py

- Python can do MPI!
 - multiple modules, mpi4py most common & mature

```Python
from mpi4py import MPI

comm = MPI.COMM_WORLD

print("Hello from rank {} of {} ...".format(comm.rank, comm.size))

# Wait for everyone to synchronise here:
comm.Barrier()
```

- Launch `n` instances of the Python interpreter:

```
$ mpirun -n 192 python myMPI-ParallelScript.py
```

- Instances share an MPI_COMM_WORLD initialised by the parallel application launcher, like any other MPI application

- Expect all relevant standard-compliant functionality to be present (point-to-point, collectives, etc.)

---

# mpi4py performance

- Sending and receiving of generic Python objects suffers from significant overheads due to  pickling & unpickling (like multiprocessing module)
 - uses lower case functions `send`, `recv`, `gather`

- Contiguous memory buffers such as Numpy arrays can be sent without pickling and with very little overhead, close to efficiency of equivalent calls from C/Fortran
 - uses upper case functions `Send`, `Recv`, `Gather`

---

# Python for HPC?

- Python plays a role in many aspects of scientific computing, and increasingly in HPC

- Only expected to grow with increasing use of AI, including coupled with traditional HPC simulation

- Tightly integrated with some applications

- Even if heaviest computational lifting done by C/C++/Fortran/CUDA, knowing how to efficiently feed & glue the core compute together with other functionality efficiently is very valuable 


##### References


Python in the NERSC Exascale Science Applications Program for Data:
https://dl.acm.org/citation.cfm?id=3149873

Exascale Deep Learning for Climate Analytics (2018 Gordon Bell Prize):
https://dl.acm.org/citation.cfm?id=3291724


---



